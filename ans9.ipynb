{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Implement a neural network to solve a multi-class classification problem and evaluate different\n",
        "weight initialization techniques.**"
      ],
      "metadata": {
        "id": "9khA7gO7Mfsr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Import Libraries"
      ],
      "metadata": {
        "id": "wTn82f-iMjVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n"
      ],
      "metadata": {
        "id": "mtLN5vcYM7Lb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Load and Prepare Iris Dataset"
      ],
      "metadata": {
        "id": "iiZx30nNM_ZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Convert labels to one-hot encoding for multi-class classification\n",
        "encoder = OneHotEncoder()\n",
        "y = encoder.fit_transform(y.reshape(-1, 1)).toarray()\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "vtk81MAZNByd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Implement Neural Network"
      ],
      "metadata": {
        "id": "aWC4cyIlNFAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # Weight initialization techniques\n",
        "        self.weight_init_techniques = {\n",
        "            'random': self.random_init,\n",
        "            'xavier': self.xavier_init,\n",
        "            'he': self.he_init\n",
        "        }\n",
        "\n",
        "        # Initialize weights and biases\n",
        "        self.weights1, self.bias1, self.weights2, self.bias2 = self.weight_init_techniques['random']()\n",
        "\n",
        "    def random_init(self):\n",
        "        weights1 = np.random.rand(self.input_dim, self.hidden_dim)\n",
        "        bias1 = np.zeros((1, self.hidden_dim))\n",
        "        weights2 = np.random.rand(self.hidden_dim, self.output_dim)\n",
        "        bias2 = np.zeros((1, self.output_dim))\n",
        "        return weights1, bias1, weights2, bias2\n",
        "\n",
        "    def xavier_init(self):\n",
        "        weights1 = np.random.randn(self.input_dim, self.hidden_dim) * np.sqrt(1 / self.input_dim)\n",
        "        bias1 = np.zeros((1, self.hidden_dim))\n",
        "        weights2 = np.random.randn(self.hidden_dim, self.output_dim) * np.sqrt(1 / self.hidden_dim)\n",
        "        bias2 = np.zeros((1, self.output_dim))\n",
        "        return weights1, bias1, weights2, bias2\n",
        "\n",
        "    def he_init(self):\n",
        "        weights1 = np.random.randn(self.input_dim, self.hidden_dim) * np.sqrt(2 / self.input_dim)\n",
        "        bias1 = np.zeros((1, self.hidden_dim))\n",
        "        weights2 = np.random.randn(self.hidden_dim, self.output_dim) * np.sqrt(2 / self.hidden_dim)\n",
        "        bias2 = np.zeros((1, self.output_dim))\n",
        "        return weights1, bias1, weights2, bias2\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def relu(self, x):\n",
        "        return np.maximum(x, 0)\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exp_x = np.exp(x)\n",
        "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.hidden_layer = np.dot(X, self.weights1) + self.bias1\n",
        "        self.hidden_layer = self.relu(self.hidden_layer)\n",
        "\n",
        "        self.output_layer = np.dot(self.hidden_layer, self.weights2) + self.bias2\n",
        "        self.output_layer = self.softmax(self.output_layer)\n",
        "\n",
        "        return self.output_layer\n",
        "\n",
        "    def cross_entropy_loss(self, predictions, labels):\n",
        "        return -np.mean(np.sum(labels * np.log(predictions), axis=1))\n",
        "\n",
        "    def train(self, X, y, learning_rate=0.01, epochs=1000):\n",
        "        for epoch in range(epochs):\n",
        "            predictions = self.forward(X)\n",
        "            loss = self.cross_entropy_loss(predictions, y)\n",
        "\n",
        "            # Backpropagation\n",
        "            d_output = predictions - y\n",
        "            d_weights2 = np.dot(self.hidden_layer.T, d_output)\n",
        "            d_bias2 = np.sum(d_output, axis=0, keepdims=True)\n",
        "\n",
        "            d_hidden_layer = d_output.dot(self.weights2.T) * (self.hidden_layer > 0).astype(int)\n",
        "            d_weights1 = np.dot(X.T, d_hidden_layer)\n",
        "            d_bias1 = np.sum(d_hidden_layer, axis=0, keepdims=True)\n",
        "\n",
        "            # Update weights and biases\n",
        "            self.weights1 -= learning_rate * d_weights1\n",
        "            self.bias1 -= learning_rate * d_bias1\n",
        "            self.weights2 -= learning_rate * d_weights2\n",
        "            self.bias2 -= learning_rate * d_bias2\n",
        "\n",
        "            if epoch % 100 == 0:\n",
        "                print(f'Epoch {epoch+1}, Loss: {loss}')\n",
        "\n",
        "    def evaluate(self, X, y):\n",
        "        predictions = self.forward(X)\n",
        "        predicted_classes = np.argmax(predictions, axis=1)\n",
        "        actual_classes = np.argmax(y, axis=1)\n",
        "        accuracy = np.mean(predicted_classes == actual_classes)\n",
        "        return accuracy\n"
      ],
      "metadata": {
        "id": "fpNcKr6xNFrs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Evaluate Different Weight Initialization Techniques"
      ],
      "metadata": {
        "id": "cXsKo7M6NHEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize neural networks with different weight initialization techniques\n",
        "nn_random = NeuralNetwork(4, 10, 3)\n",
        "nn_xavier = NeuralNetwork(4, 10, 3)\n",
        "nn_he = NeuralNetwork(4, 10, 3)\n",
        "\n",
        "# Set different initialization techniques\n",
        "nn_random.weights1, nn_random.bias1, nn_random.weights2, nn_random.bias2 = nn_random.weight_init_techniques['random']()\n",
        "nn_xavier.weights1, nn_xavier.bias1, nn_xavier.weights2, nn_xavier.bias2 = nn_xavier.weight_init_techniques['xavier']()\n",
        "nn_he.weights1, nn_he.bias1, nn_he.weights2, nn_he.bias2 = nn_he.weight_init_techniques['he']()\n",
        "\n",
        "# Train neural networks\n",
        "nn_random.train(X_train, y_train)\n",
        "nn_xavier.train(X_train, y_train)\n",
        "nn_he.train(X_train, y_train)\n",
        "\n",
        "# Evaluate neural networks\n",
        "accuracy_random = nn_random.evaluate(X_test, y_test)\n",
        "accuracy_xavier = nn_xavier.evaluate(X_test, y_test)\n",
        "accuracy_he = nn_he.evaluate(X_test, y_test)\n",
        "\n",
        "print(f\"Random Initialization Accuracy: {accuracy_random}\")\n",
        "print(f\"Xavier Initialization Accuracy: {accuracy_xavier}\")\n",
        "print(f\"He Initialization Accuracy: {accuracy_he}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xr2n8rejNJpE",
        "outputId": "924d9115-54d0-4975-82e2-2d99b0de9503"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 15.667858768002098\n",
            "Epoch 101, Loss: 1.0984039336279603\n",
            "Epoch 201, Loss: 1.0984039336279603\n",
            "Epoch 301, Loss: 1.0984039336279603\n",
            "Epoch 401, Loss: 1.0984039336279603\n",
            "Epoch 501, Loss: 1.0984039336279603\n",
            "Epoch 601, Loss: 1.0984039336279603\n",
            "Epoch 701, Loss: 1.0984039336279603\n",
            "Epoch 801, Loss: 1.0984039336279603\n",
            "Epoch 901, Loss: 1.0984039336279603\n",
            "Epoch 1, Loss: 5.0731876128667865\n",
            "Epoch 101, Loss: 1.0984039336279603\n",
            "Epoch 201, Loss: 1.0984039336279603\n",
            "Epoch 301, Loss: 1.0984039336279603\n",
            "Epoch 401, Loss: 1.0984039336279603\n",
            "Epoch 501, Loss: 1.0984039336279603\n",
            "Epoch 601, Loss: 1.0984039336279603\n",
            "Epoch 701, Loss: 1.0984039336279603\n",
            "Epoch 801, Loss: 1.0984039336279603\n",
            "Epoch 901, Loss: 1.0984039336279603\n",
            "Epoch 1, Loss: 2.2797051965483432\n",
            "Epoch 101, Loss: 1.0984039336279603\n",
            "Epoch 201, Loss: 1.0984039336279603\n",
            "Epoch 301, Loss: 1.0984039336279603\n",
            "Epoch 401, Loss: 1.0984039336279603\n",
            "Epoch 501, Loss: 1.0984039336279603\n",
            "Epoch 601, Loss: 1.0984039336279603\n",
            "Epoch 701, Loss: 1.0984039336279603\n",
            "Epoch 801, Loss: 1.0984039336279603\n",
            "Epoch 901, Loss: 1.0984039336279603\n",
            "Random Initialization Accuracy: 0.3\n",
            "Xavier Initialization Accuracy: 0.3\n",
            "He Initialization Accuracy: 0.3\n"
          ]
        }
      ]
    }
  ]
}