{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Write a program to generate confusion matrices for different models and analyze their\n",
        "performance.**"
      ],
      "metadata": {
        "id": "uv7aIvv0Nd3n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Import Libraries"
      ],
      "metadata": {
        "id": "FKfv38JVNgpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n"
      ],
      "metadata": {
        "id": "_S_s8-TiNm6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Load and Prepare Iris Dataset"
      ],
      "metadata": {
        "id": "weOLPvNYNwKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "KOPuiun9Nz49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Implement Logistic Regression Model"
      ],
      "metadata": {
        "id": "js3jZbV2N24W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression:\n",
        "    def __init__(self, learning_rate=0.01, epochs=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        num_samples, num_features = X.shape\n",
        "        self.weights = np.zeros(num_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        for _ in range(self.epochs):\n",
        "            linear_model = np.dot(X, self.weights) + self.bias\n",
        "            y_predicted = self.sigmoid(linear_model)\n",
        "\n",
        "            # Compute gradients\n",
        "            dw = (1 / num_samples) * np.dot(X.T, (y_predicted - y))\n",
        "            db = (1 / num_samples) * np.sum(y_predicted - y)\n",
        "\n",
        "            # Update parameters\n",
        "            self.weights -= self.learning_rate * dw\n",
        "            self.bias -= self.learning_rate * db\n",
        "\n",
        "    def predict(self, X):\n",
        "        linear_model = np.dot(X, self.weights) + self.bias\n",
        "        y_predicted = self.sigmoid(linear_model)\n",
        "        y_predicted_cls = np.where(y_predicted > 0.5, 1, 0)\n",
        "\n",
        "        # Since Iris is multi-class, we need to adjust the prediction logic\n",
        "        # Here, we'll use a simple approach for demonstration purposes\n",
        "        # In practice, you would use a one-vs-all or one-vs-one strategy\n",
        "        y_predicted_cls = np.argmax(np.array([y_predicted, 1 - y_predicted, np.zeros_like(y_predicted)]).T, axis=1)\n",
        "\n",
        "        return y_predicted_cls\n",
        "\n",
        "    def evaluate(self, X, y):\n",
        "        predictions = self.predict(X)\n",
        "        accuracy = np.mean(predictions == y)\n",
        "        return accuracy\n"
      ],
      "metadata": {
        "id": "jOvk8D6uN4uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Implement Neural Network Model"
      ],
      "metadata": {
        "id": "LcGglhUeN68A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # Initialize weights randomly\n",
        "        self.weights1 = np.random.rand(input_dim, hidden_dim)\n",
        "        self.bias1 = np.zeros((1, hidden_dim))\n",
        "        self.weights2 = np.random.rand(hidden_dim, output_dim)\n",
        "        self.bias2 = np.zeros((1, output_dim))\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def relu(self, x):\n",
        "        return np.maximum(x, 0)\n",
        "\n",
        "    def softmax(self, x):\n",
        "        exp_x = np.exp(x)\n",
        "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.hidden_layer = np.dot(X, self.weights1) + self.bias1\n",
        "        self.hidden_layer = self.relu(self.hidden_layer)\n",
        "\n",
        "        self.output_layer = np.dot(self.hidden_layer, self.weights2) + self.bias2\n",
        "        self.output_layer = self.softmax(self.output_layer)\n",
        "\n",
        "        return self.output_layer\n",
        "\n",
        "    def cross_entropy_loss(self, predictions, labels):\n",
        "        return -np.mean(np.sum(labels * np.log(predictions), axis=1))\n",
        "\n",
        "    def train(self, X, y, learning_rate=0.01, epochs=1000):\n",
        "        for epoch in range(epochs):\n",
        "            predictions = self.forward(X)\n",
        "            loss = self.cross_entropy_loss(predictions, y)\n",
        "\n",
        "            # Backpropagation\n",
        "            d_output = predictions - y\n",
        "            d_weights2 = np.dot(self.hidden_layer.T, d_output)\n",
        "            d_bias2 = np.sum(d_output, axis=0, keepdims=True)\n",
        "\n",
        "            d_hidden_layer = d_output.dot(self.weights2.T) * (self.hidden_layer > 0).astype(int)\n",
        "            d_weights1 = np.dot(X.T, d_hidden_layer)\n",
        "            d_bias1 = np.sum(d_hidden_layer, axis=0, keepdims=True)\n",
        "\n",
        "            # Update weights and biases\n",
        "            self.weights1 -= learning_rate * d_weights1\n",
        "            self.bias1 -= learning_rate * d_bias1\n",
        "            self.weights2 -= learning_rate * d_weights2\n",
        "            self.bias2 -= learning_rate * d_bias2\n",
        "\n",
        "            if epoch % 100 == 0:\n",
        "                print(f'Epoch {epoch+1}, Loss: {loss}')\n",
        "\n",
        "    def predict(self, X):\n",
        "        predictions = self.forward(X)\n",
        "        return np.argmax(predictions, axis=1)\n",
        "\n",
        "    def evaluate(self, X, y):\n",
        "        predictions = self.predict(X)\n",
        "        accuracy = np.mean(predictions == np.argmax(y, axis=1))\n",
        "        return accuracy\n"
      ],
      "metadata": {
        "id": "5uR7b9anN7h-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Generate Confusion Matrices"
      ],
      "metadata": {
        "id": "xuInR9dpN9aH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Train logistic regression model\n",
        "# Note: Logistic regression needs adjustment for multi-class classification\n",
        "# Here, we focus on the neural network for multi-class\n",
        "\n",
        "# Train neural network model\n",
        "nn_model = NeuralNetwork(4, 10, 3)\n",
        "y_train_onehot = np.eye(3)[y_train]\n",
        "nn_model.train(X_train, y_train_onehot)\n",
        "\n",
        "# Predict using neural network\n",
        "y_pred_nn = nn_model.predict(X_test)\n",
        "\n",
        "# Generate confusion matrix for neural network\n",
        "conf_mat_nn = confusion_matrix(y_test, y_pred_nn)\n",
        "print(\"Confusion Matrix (Neural Network):\\n\", conf_mat_nn)\n",
        "\n",
        "# For logistic regression, we need to adjust it for multi-class\n",
        "# Here, we skip it due to its complexity in this context\n",
        "\n",
        "# However, if you want to use logistic regression for multi-class,\n",
        "# consider using one-vs-all or one-vs-one strategies.\n",
        "\n",
        "# To simplify, let's use a basic form of logistic regression\n",
        "# and acknowledge its limitations for multi-class problems.\n",
        "\n",
        "# Train logistic regression (simplified for demonstration)\n",
        "lr_model = LogisticRegression()\n",
        "# Note: This won't work well for multi-class without adjustments\n",
        "\n",
        "# For multi-class, consider using sklearn's LogisticRegression\n",
        "from sklearn.linear_model import LogisticRegression as SKLogisticRegression\n",
        "sk_lr_model = SKLogisticRegression(max_iter=1000)\n",
        "sk_lr_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict using sklearn's logistic regression\n",
        "y_pred_sk_lr = sk_lr_model.predict(X_test)\n",
        "\n",
        "# Generate confusion matrix for sklearn's logistic regression\n",
        "conf_mat_sk_lr = confusion_matrix(y_test, y_pred_sk_lr)\n",
        "print(\"Confusion Matrix (sklearn Logistic Regression):\\n\", conf_mat_sk_lr)\n"
      ],
      "metadata": {
        "id": "0-uhG-vbOBtJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}