{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Implement a neural network for binary classification and test different activation functions like\n",
        "ReLU, Sigmoid, and Tanh.**"
      ],
      "metadata": {
        "id": "vMRDYXJILwWE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Import Libraries"
      ],
      "metadata": {
        "id": "JQF3jXs1Lxjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "iMCDjy-UL6Uo"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Load and Prepare Iris Dataset"
      ],
      "metadata": {
        "id": "e6L2qAH7L9pF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Iris dataset\n",
        "df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None)\n",
        "\n",
        "# Select two classes for binary classification\n",
        "df = df.iloc[:100, :]  # Select first 100 rows (setosa and versicolor)\n",
        "\n",
        "# Convert labels to binary (0 for versicolor, 1 for setosa)\n",
        "y = np.where(df.iloc[:, 4].values == 'Iris-setosa', 1.0, 0.0).reshape(-1, 1)\n",
        "\n",
        "# Features\n",
        "X = df.iloc[:, :4].values\n"
      ],
      "metadata": {
        "id": "iBzQbP5nL-Qk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Implement Neural Network"
      ],
      "metadata": {
        "id": "kBuxCcRTMB-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        # Initialize weights randomly\n",
        "        self.weights1 = np.random.rand(input_dim, hidden_dim)\n",
        "        self.weights2 = np.random.rand(hidden_dim, output_dim)\n",
        "\n",
        "        # Initialize biases to zero\n",
        "        self.bias1 = np.zeros((1, hidden_dim))\n",
        "        self.bias2 = np.zeros((1, output_dim))\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def relu(self, x):\n",
        "        return np.maximum(x, 0)\n",
        "\n",
        "    def tanh(self, x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "    def forward(self, X, activation='sigmoid'):\n",
        "        self.hidden_layer = np.dot(X, self.weights1) + self.bias1\n",
        "\n",
        "        if activation == 'sigmoid':\n",
        "            self.hidden_layer = self.sigmoid(self.hidden_layer)\n",
        "        elif activation == 'relu':\n",
        "            self.hidden_layer = self.relu(self.hidden_layer)\n",
        "        elif activation == 'tanh':\n",
        "            self.hidden_layer = self.tanh(self.hidden_layer)\n",
        "\n",
        "        self.output_layer = np.dot(self.hidden_layer, self.weights2) + self.bias2\n",
        "        self.output_layer = self.sigmoid(self.output_layer)  # Output layer always uses sigmoid for binary classification\n",
        "\n",
        "        return self.output_layer\n",
        "\n",
        "    def train(self, X, y, learning_rate=0.01, epochs=1000, activation='sigmoid'):\n",
        "        for epoch in range(epochs):\n",
        "            output = self.forward(X, activation)\n",
        "            error = y - output\n",
        "\n",
        "            # Backpropagation\n",
        "            d_output = error * output * (1 - output)\n",
        "            d_weights2 = np.dot(self.hidden_layer.T, d_output)\n",
        "            d_bias2 = np.sum(d_output, axis=0, keepdims=True)\n",
        "\n",
        "            if activation == 'sigmoid':\n",
        "                d_hidden_layer = d_output.dot(self.weights2.T) * self.hidden_layer * (1 - self.hidden_layer)\n",
        "            elif activation == 'relu':\n",
        "                d_hidden_layer = d_output.dot(self.weights2.T) * (self.hidden_layer > 0).astype(int)\n",
        "            elif activation == 'tanh':\n",
        "                d_hidden_layer = d_output.dot(self.weights2.T) * (1 - self.hidden_layer**2)\n",
        "\n",
        "            d_weights1 = np.dot(X.T, d_hidden_layer)\n",
        "            d_bias1 = np.sum(d_hidden_layer, axis=0, keepdims=True)\n",
        "\n",
        "            # Update weights and biases\n",
        "            self.weights1 += learning_rate * d_weights1\n",
        "            self.bias1 += learning_rate * d_bias1\n",
        "            self.weights2 += learning_rate * d_weights2\n",
        "            self.bias2 += learning_rate * d_bias2\n",
        "\n",
        "            if epoch % 100 == 0:\n",
        "                print(f'Epoch {epoch+1}, Loss: {np.mean((y - output)**2)}')\n",
        "\n",
        "    def evaluate(self, X, y):\n",
        "        predictions = self.forward(X)\n",
        "        predictions = (predictions > 0.5).astype(int)\n",
        "        accuracy = np.mean(predictions == y)\n",
        "        return accuracy\n"
      ],
      "metadata": {
        "id": "5dGJaFzoMDh6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Test Different Activation Functions"
      ],
      "metadata": {
        "id": "YwKXkGF6MGDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize neural networks with different activation functions\n",
        "nn_sigmoid = NeuralNetwork(4, 10, 1)\n",
        "nn_relu = NeuralNetwork(4, 10, 1)\n",
        "nn_tanh = NeuralNetwork(4, 10, 1)\n",
        "\n",
        "# Train neural networks\n",
        "nn_sigmoid.train(X, y, activation='sigmoid')\n",
        "nn_relu.train(X, y, activation='relu')\n",
        "nn_tanh.train(X, y, activation='tanh')\n",
        "\n",
        "# Evaluate neural networks\n",
        "accuracy_sigmoid = nn_sigmoid.evaluate(X, y)\n",
        "accuracy_relu = nn_relu.evaluate(X, y)\n",
        "accuracy_tanh = nn_tanh.evaluate(X, y)\n",
        "\n",
        "print(f\"Sigmoid Accuracy: {accuracy_sigmoid}\")\n",
        "print(f\"ReLU Accuracy: {accuracy_relu}\")\n",
        "print(f\"Tanh Accuracy: {accuracy_tanh}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LX3k9IKaMH1n",
        "outputId": "f628ddc0-59bb-4b86-d615-7f69f0027fde"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.49598409953379347\n",
            "Epoch 101, Loss: 0.25080211415228965\n",
            "Epoch 201, Loss: 0.1870413799669009\n",
            "Epoch 301, Loss: 0.037403017929001206\n",
            "Epoch 401, Loss: 0.01661206575204911\n",
            "Epoch 501, Loss: 0.01023741769103927\n",
            "Epoch 601, Loss: 0.007282812259871212\n",
            "Epoch 701, Loss: 0.0056058197961863\n",
            "Epoch 801, Loss: 0.004533036795627405\n",
            "Epoch 901, Loss: 0.003790197713903005\n",
            "Epoch 1, Loss: 0.49999999999797795\n",
            "Epoch 101, Loss: 0.49999999999797795\n",
            "Epoch 201, Loss: 0.49999999999797795\n",
            "Epoch 301, Loss: 0.49999999999797795\n",
            "Epoch 401, Loss: 0.49999999999797795\n",
            "Epoch 501, Loss: 0.49999999999797795\n",
            "Epoch 601, Loss: 0.49999999999797795\n",
            "Epoch 701, Loss: 0.49999999999797795\n",
            "Epoch 801, Loss: 0.49999999999797795\n",
            "Epoch 901, Loss: 0.49999999999797795\n",
            "Epoch 1, Loss: 0.4939566967834169\n",
            "Epoch 101, Loss: 0.18078868344614005\n",
            "Epoch 201, Loss: 0.006740710464178999\n",
            "Epoch 301, Loss: 0.0031357442661673635\n",
            "Epoch 401, Loss: 0.002014631583195918\n",
            "Epoch 501, Loss: 0.0014763090408593626\n",
            "Epoch 601, Loss: 0.0011619841964367289\n",
            "Epoch 701, Loss: 0.0009565657334850721\n",
            "Epoch 801, Loss: 0.0008120801051223086\n",
            "Epoch 901, Loss: 0.0007050480627099477\n",
            "Sigmoid Accuracy: 1.0\n",
            "ReLU Accuracy: 0.5\n",
            "Tanh Accuracy: 0.5\n"
          ]
        }
      ]
    }
  ]
}